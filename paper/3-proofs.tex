\section[Unverfied implementation]{Unverfied implementation\implementation{TwoPassMerge.NoProofs}} \label{sec:no-proofs}
Let us begin by implementing the described algorithms without any proof of their correctness. We define \texttt{Heap} datatype as:

\begin{code}
data Heap : Set where
  empty : Heap
  node  : Priority → Rank → Heap → Heap → Heap
\end{code}
\noindent
According to this definition a heap is either empty or it is a node with priority, rank and two subheaps. Both \texttt{Priority} and \texttt{Rank} are aliases to \texttt{Nat} which allows us to perform on them any operation that works on \texttt{Nat} type. Note that storing rank in a node is redundant -- we could just compute size of a tree whenever necessary. The reason why I chose to store rank in the constructor is that it will be instructive to show in Section \ref{sec:rank-property} how it is converted into inductive type family index.

\begin{listing}[htb!]
\begin{code}
makeT : Priority → Heap → Heap → Heap
makeT p l r with rank l ≥ rank r
makeT p l r | true  = node p (suc (rank l + rank r)) l r
makeT p l r | false = node p (suc (rank l + rank r)) r l

merge : Heap → Heap → Heap
merge empty h2 = h2
merge h1 empty = h1
merge (node p1 h1-r l1 r1) (node p2 h2-r l2 r2)
  with p1 < p2
merge (node p1 h1-r l1 r1) (node p2 h2-r l2 r2)
  | true  = makeT p1 l1 (merge r1 (node p2 h2-r l2 r2))
merge (node p1 h1-r l1 r1) (node p2 h2-r l2 r2)
  | false = makeT p2 l2 (merge (node p1 h1-r l1 r1) r2)
\end{code}
\caption{Implementation of makeT and merge}\label{lst:makeT-merge}
\end{listing}

\subsection{merge}\label{sec:twopass-merge}

Heaps are merged using a recursive algorithm. We need to consider four cases:

\begin{enumerate}
 \item (base case) \texttt{h1} is empty: return \texttt{h2}.
 \item (base case) \texttt{h2} is empty: return \texttt{h1}.
 \item (inductive case) priority \texttt{p1} is higher than \texttt{p2}: \texttt{p1} becomes new root, \texttt{l1} becomes its one child and \texttt{r1}$\oplus$\texttt{h2} becomes the other.
 \item (inductive case) priority \texttt{p2} is higher than or equal to \texttt{p1}: \texttt{p2} becomes new root, \texttt{l2} becomes its one child and \texttt{r2}$\oplus$\texttt{h1} becomes the other.
\end{enumerate}
\noindent
There is no guarantee that \textit{r1}$\oplus$\textit{h2} (or \textit{r2}$\oplus$\textit{h1}) will be smaller than \textit{l1} (or \textit{l2}). To ensure that rank invariant is maintained we use a helper function \texttt{makeT}, as proposed by Okasaki \cite{Oka99}. We pass new children and the priority to \texttt{makeT}, which creates a new node with the given priority and swaps the children if necessary (see \Listing{lst:makeT-merge}). As Okasaki points out this algorithm can be view as having two passes: a top-down pass that performs merging and a bottom-up pass that restores the rank invariant.

\subsection{insert}

Insert can now be defined in terms of merging with a singleton heap as described in Section~\ref{sec:wblh}. See companion code for implementation.

\subsection{findMin and deleteMin}

To retrieve element with the highest priority we return value stored in the root of a heap:

\begin{code}
findMin : Heap → Priority
findMin empty          = \hilight{\{ \}?}
findMin (node p _ _ _) = p
\end{code}
\noindent
Here we encounter a problem: what should \texttt{findMin} return for an empty heap? If we were using a language like Haskell or ML one thing we could consider is throwing an error. This is the choice made by Okasaki in ``Purely Functional Data Structures''. But throwing an exception is precisely the thing that would make our implementation impure! Besides Agda is a total language, which means that every function must terminate with a result. Throwing error is therefore not an option. Another alternative is to assume a default priority that will be returned for an empty heap. This priority would have to be some distinguished natural number. $0$ represents the highest priority so it is unreasonable to assume it as default. We could return $\infty$, which represents priority lower than any natural number. This would require us to extend definition of \texttt{Nat} with $\infty$, which in turn would force us to modify all functions that pattern match on values of \texttt{Nat}. Redefining natural numbers for the sake of getting one function right also does not sound like a good option. Let's face it -- our types don't reflect that \texttt{findMin} function is not defined for an empty heap! To solve this problem we need to be more specific about types. One solution would be to use \texttt{Maybe} datatype:

\begin{code}
data Maybe (A : Set) : Set where
  nothing : Maybe A
  just    : A → Maybe A

findMinM : Heap → Maybe Priority
findMinM empty          = nothing
findMinM (node p _ _ _) = just p
\end{code}

\noindent
Returning \texttt{nothing} is like saying ``no output exists for the given input data''. This allows us to express the fact that \texttt{findMin} is not defined for some input values. This solution works but it forces every caller of \texttt{findMinM} to inspect the result and be prepared for \texttt{nothing}.

The best solution to this problem is to ensure that \texttt{findMin} cannot be applied to an empty heap. We can achieve this by indexing \texttt{Heap} with its size. Luckily for us doing so will also allow to prove the rank property.

Implementation of \texttt{deleteMin} is based on description in Section~\ref{sec:wblh}. It faces the same problem as \texttt{findMin} -- it is undefined for empty heap.

\section[Prooving rank property]{Prooving rank property\implementation{TwoPassMerge.RankProof}} \label{sec:rank-property}

We will now prove that our implementation maintains the rank property. The first step is to express \texttt{Rank} at the type level as an index of \texttt{Heap} datatype. Since rank of a heap is now part of its type we can ensure that rank of left subtree is not smaller than rank of the right subtree. We do this be requiring that \texttt{node} constructor is given a proof that rank invariant holds. To express such proofs we use ≥ datatype:

\begin{code}
data _≥_ : Nat → Nat → Set where
  ge0 : \{  y : Nat\} → y     ≥ zero
  geS : \{x y : Nat\} → x ≥ y → suc x ≥ suc y
\end{code}
\noindent
Values of this type, which are indexed by two natural numbers, prove that: a) any natural number is greater than or equal to \texttt{0} (\texttt{ge0} constructor); b) if two numbers are in greater-equal relation then their successors are also in that relation (\texttt{geS} constructor). This type represents concept of data as evidence~\cite{AltMcBMcK05}. We use \texttt{order} function to compare two natural numbers and \texttt{Order} datatype to express the result. Implementation is located in \texttt{Basics.Ordering} module of the companion code.

Having defined ≥ we can now give new definition of \texttt{Heap}:

\begin{code}
data Heap : Rank → Set where
  empty : Heap zero
  node  : \{l r : Rank\} → Priority → l ≥ r →
          Heap l → Heap r → Heap (suc (l + r))
\end{code}

\noindent
Empty heap contains no elements and so \texttt{empty} returns \texttt{Heap} indexed with \texttt{0}. Non-empty node stores an element and two children of rank \textit{l} and \textit{r}. Therefore the size of the resulting heap is $1 + l + r$, which we express as $\suc(l + r)$. We must also supply a value of type \texttt{l ≥ r} to the constructor, ie. we must provide a proof that rank invariant holds.

Proving the rank invariant itself is surprisingly simple. We can easily obtain evidence that rank of left subtree is not smaller than rank of right subtree by using \texttt{order} function, that compares two \texttt{Nat}s and supplies a proof of the result. But there is a another difficulty here. Since we index heaps by their sizes we now require that \texttt{makeT} and \texttt{merge} construct trees of correct size. We have to show that the size of merged heap is equal to the sum of sizes of heaps being merged. Recall that our merging algorithm is two pass: we use \texttt{merge} to actually do the merging and \texttt{makeT} to
restore the rank invariant if necessary. We will therefore conduct the proof in two stages by showing that: a) \texttt{makeT} creates a node of required size; b) recursive calls to \texttt{merge} produce heaps of required size.

\subsection{Proving makeT}\label{sec:twopass-makeT-proof}

\texttt{makeT} takes subtrees of rank \textit{l} and \textit{r} and produces a new tree with rank $\suc(l + r)$. We must prove that each of two cases of \texttt{makeT} returns tree of correct size:

\begin{enumerate}
 \item If rank \texttt{l} is greater than or equal to rank \textbf{r} then no extra proof is necessary as everything follows from the definition of +.
 \item If rank \texttt{r} is greater than or equal to rank \textbf{l} then we must swap left and right subtrees. This requires us to prove that:

\begin{equation*}
suc (r + l) ≡ suc (l + r)
\end{equation*}

That proof is done using congruence on $\suc$ function and commutativity of addition. We will define that proof as \texttt{makeT-lemma} as we will be using in subsequent proofs.
\end{enumerate}
\noindent
New code of \texttt{makeT} is show in \Listing{lst:rank-proof-makeT-two-pass}. Note the use of \texttt{subst}. We use it to apply the proof to the \texttt{Heap} type constructor and convert the type produced by \texttt{(node p r≥l r l)} expression into the type given in type signature. % not exactly precise - type signature is transformed using definitions.

\begin{listing}[thb!]
\begin{code}
makeT-lemma : (a b : Nat) → suc (a + b) ≡ suc (b + a)
makeT-lemma a b = cong suc (+comm a b)

makeT : \{l r : Rank\} → Priority → Heap l → Heap r → Heap (suc (l + r))
makeT \{l-rank\} \{r-rank\} p l r with order l-rank r-rank
makeT \{l-rank\} \{r-rank\} p l r | ge l≥r
  = node p l≥r l r
makeT \{l-rank\} \{r-rank\} p l r | le r≥l
  = subst Heap (makeT-lemma r-rank l-rank) (node p r≥l r l)
\end{code}
\caption{Implementation of \texttt{makeT} with verified rank property.}\label{lst:rank-proof-makeT-two-pass}
\end{listing}

\subsection{Proving merge}

Now we must show that all four cases of merge -- shown in \Listing{lst:makeT-merge} -- produce heap of required size.

\subsubsection{base cases}

In the first base case we have $h1 ≡ 0$. Therefore:

\begin{equation*}
h1 + h2 ≡ 0 + h2 \stackrel{+, (1)}{≡} h2
\end{equation*}
\noindent
Which ends the first proof -- everything follows from definition of $+$\footnote{The $\stackrel{+, (1)}{≡}$ notation means that equality follows from first defining equation of $+$.}. In the second base case $h2 ≡ 0$ and things are slightly more difficult: the definition of $+$ only says that $0$ is the left identity, but it doesn't say that it is also the right identity. We must therefore construct a proof that:

\begin{equation*}
h1 + 0 \stackrel{?}{≡} h1
\end{equation*}
\noindent
Luckily for us, we already have that proof defined in the \texttt{Basics.Reasoning} module as \texttt{+0}. The only problem is that our proof is in the opposite direction. It proves $a ≡ a + 0$, not $a + 0 ≡ a$. We solve that using symmetry of $≡$.% Code for the base cases is shown in \Listing{lst:rank-proof-merge-two-pass}.

\subsubsection{inductive cases}\label{sec:twopass-merge-inductive}

In an indective case we know that neither \texttt{h1} nor \texttt{h2} is empty, ie. their sizes are given as $\suc (l1 + r1)$ and $\suc (l2 + r2)$ respectively. This means that Agda sees expected size of the merged heap as:

\begin{equation*}
\suc (l1 + r1) + \suc (l2 + r2) \stackrel{+, (2)}{≡} \suc ((l1 + r1) + \suc (l2 + r2))
\end{equation*}
\noindent
This will be our goal in both proofs of inductive cases.

In the first inductive case we construct the result by calling\footnote{Note that \texttt{node} constructor in the unverified implementation show in \Listing{lst:makeT-merge} takes slightly different parameters. This is because we changed the definition of \texttt{Heap} datatype to take the proof of rank property instead of storing the rank in the constructor.}:

\begin{code}
makeT p1 l1 (merge r1 (node p2 l2≥r2 l2 r2))
\end{code}
\noindent
Call to \texttt{node} with \texttt{l2} and \texttt{r2} as parameters produces node of rank $\suc(l2 + r2)$. Passing it to \texttt{merge} together with \texttt{r1} gives a tree of rank $r1 + \suc(l2 + r2)$ (by the type signature of \texttt{merge}). Passing result of \texttt{merge} to \texttt{makeT} produces tree of rank $\suc (l1 + (r1 + \suc (l2 + r2)))$ by the type signature of \texttt{makeT}. We must therefore construct a proof that:

\begin{equation*}
\suc (l1 + (r1 + \suc (l2 + r2))) ≡ \suc ((l1 + r1) + \suc (l2 + r2))
\end{equation*}
\noindent
Appealing to congruence on $\suc$ leaves us with a proof of:

\begin{equation*}
l1 + (r1 + \suc (l2 + r2)) ≡ (l1 + r1) + \suc (l2 + r2)
\end{equation*}
\noindent
Substituting $a = l1$, $b = r1$ and $c = suc (l2 + r2)$ gives:

\begin{equation*}
a + (b + c) ≡ (a + b) + c
\end{equation*}
\noindent
This is associativity of addition, that we already proved in \texttt{Basics.Reasoning}.

The proof of second inductive case is much more interesting. This time we construct the resulting node by calling:

\begin{code}
makeT p2 l2 (merge r2 (node p1 l1≥r1 l1 r1))
\end{code}
\noindent
and therefore have to prove that:

\begin{equation*}
\suc (l2 + (r2 + \suc (l1 + r1))) ≡ \suc ((l1 + r1) + \suc (l2 + r2))
\end{equation*}
\noindent
Again we use congruence to deal with the outer calls to $\suc$ and substitute $a = l2$, $b = r2$ and $c = l1 + r1$. This leaves us with a proof of lemma A:

\begin{equation*}
a + (b + \suc c) ≡ c + \suc (a + b)
\end{equation*}
\noindent
From associativity we know that:

\begin{equation*}
a + (b + \suc c) ≡ (a + b) + \suc c
\end{equation*}
\noindent
If we prove lemma B:

\begin{equation*}
(a + b) + \suc c ≡ c + \suc (a + b)
\end{equation*}
\noindent
then we can combine lemmas A and B using transitivity to get the final proof. We substitute $n = a + b$ and $m = c$ and rewrite lemma B as:

\begin{equation*}
n + \suc m ≡ m + \suc n
\end{equation*}
\noindent
From symmetry of \texttt{+suc} we know that:

\begin{equation*}
n + \suc m ≡ \suc (n + m)
\end{equation*}
\noindent
Using transitivity we combine it with congruence on commutativity of addition to prove:

\begin{equation*}
\suc (n + m) ≡ \suc (m + n)
\end{equation*}
\noindent
Again, using transitivity we combine it with \texttt{+suc} to show:

\begin{equation*}
\suc (m + n) ≡ m + \suc n
\end{equation*}
\noindent
Which proves lemma B and therefore the whole proof is complete (\Listing{lst:twopass-merge-2nd-proof}, see companion code for complete code):

\begin{listing}[thb!]
\begin{code}
lemma-B : (n m : Nat) → n + suc m ≡ m + suc n
lemma-B n m = trans (sym (+suc n m)) (trans (cong suc (+comm n m)) (+suc m n))

lemma-A : (a b c : Nat) → a + (b + suc c) ≡ c + suc (a + b)
lemma-A a b c = trans (+assoc a b (suc c)) (lemma-B (a + b) c)

proof-2 : (l1 r1 l2 r2 : Nat) → suc (l2 + (r2  + suc (l1 + r1)))
                              ≡ suc ((l1 + r1) + suc (l2 + r2))
proof-2 l1 r1 l2 r2 = cong suc (lemma-A l2 r2 (l1 + r1))
\end{code}
\caption{Proof of second inductive case of \texttt{merge}.}\label{lst:twopass-merge-2nd-proof}
\end{listing}

\subsection{insert}

We require that inserting an element into the heap increases its rank by one. Now that rank is encoded as datatype index this fact can be reflected in the type signatures of \texttt{insert}. As previously we define insert as merge with a singleton heap. Size of singleton heap is 1 (ie. \texttt{suc zero}), while already existing heap has size n. According to definition of merge the resulting heap will therefore have size:

\begin{equation}
(\suc \zero) + n \stackrel{+, (2)}{≡} \suc (\zero + n) \stackrel{+, (1)}{≡} \suc n
\end{equation}
\noindent
Which is the size we require in the type signature of \texttt{insert}. This means we don't need any additional proof because expected result follows from definitions.

\subsection{findMin, deleteMin}

Having encoded rank at the type level we can now write total versions of \texttt{findMin} and \texttt{deleteMin}. By requiring that input \texttt{Heap} has rank \texttt{suc n} we exclude the possibility of passing empty heap to any of these functions.

\section{Constructing equality proofs using transitivity}\label{sec:eq-proofs-using-trans}

Now that we have conducted an inductive prove of merge in subsection \ref{sec:twopass-merge-inductive} we can focus on a general technique used in that proof. Let us rewrite \texttt{proof-2} in a different way to see closely what is happening at each step. Inlining lemmas A and B into \texttt{proof-2} gives:

\begin{code}
proof-2i : (l1 r1 l2 r2 : Nat) → suc (l2 + (r2  + suc (l1 + r1)))
                               ≡ suc ((l1 + r1) + suc (l2 + r2))
proof-2i l1 r1 l2 r2 =
  cong suc (trans (+assoc l2 r2 (suc (l1 + r1)))
           (trans (sym (+suc (l2 + r2) (l1 + r1)))
           (trans (cong suc (+comm (l2 + r2) (l1 + r1)))
                  (+suc (l1 + r1) (l2 + r2))))
\end{code}

We see a lot of properties combined using transitivity. In general, if we have to prove $a ≡ e$ and we can prove $a ≡ b$ using $\prof 1$, $b ≡ c$ using $\prof 2$, $c ≡ d$ using $\prof 3$, $d ≡ e$ using $\prof 4$ then we can combine these proofs using transitivity to get our final proof:

\begin{equation*}
\trans\, (\prof 1)\, (\trans\, (\prof 2)\, (\trans\, (\prof 3)\, (\prof 4)))
\end{equation*}
\noindent
While simple to use, combining proofs with transitivity can be not so obvious at first: the intermediate terms being proved are hidden from us and we have to reconstruct them every time we read our proof. Let us then replace usage of transitivity with the following notation, which explicitly shows intermediate proof steps as well as their proofs:

\begin{align*}
a& ≡[ \prof 1 ]\\
b& ≡[ \prof 2 ]\\
c& ≡[ \prof 3 ]\\
d& ≡[ \prof 4 ]\\
e&
\end{align*}
\noindent
Rewriting \texttt{proof-2i} in this notation gives us:

\begin{align*}
                                \suc (l2 + (r2 + \suc (l1 + r1)))& ≡[ \congr\;\suc ]\\
{\color{gray} \suc(} l2 + (r2 + \suc (l1 + r1))  {\color{gray})} & ≡[\assoc\;l2\;r2\;(\suc (l1 + r1))]\\
{\color{gray} \suc(} (l2 + r2) + \suc (l1 + r1)  {\color{gray})} & ≡[ \sym (\Psuc\;(l2 + r2)\;(l1 + r1))]\\
{\color{gray} \suc(} \suc ((l2 + r2) + (l1 + r1)){\color{gray})} & ≡[ \congr\;\suc\;(\comm\;(l2 + r2)\;(l1 + r1)) ]\\
{\color{gray} \suc(} \suc ((l1 + r1) + (l2 + r2)){\color{gray})} & ≡[\Psuc\;(l1 + r1)\;(l2 + r2) ]\\
{\color{gray} \suc(} (l1 + r1) + \suc (l2 + r2)  {\color{gray})} &
\end{align*}

\noindent
We use ${\color{gray}\suc}$ to denote that everything happens under a call to \texttt{suc} (thanks to using congruence as the first proof). If you compare this notation with code of \texttt{proof-2i} you'll see that proofs in square brackets correspond to proofs with have combined using $\trans$, while series of expressions left of $≡$ parallels our reasoning conducted in Section \ref{sec:twopass-merge-inductive}.

\section[Proving rank property for single-pass merge by composing existing proofs]{Proving rank property for single-pass merge by composing existing proofs\implementation{SinglePassMerge.RankProof}} \label{sec:single-pass-merge-proof-by-comp}

As mentioned in Section \ref{sec:twopass-merge} \texttt{merge} can be viewed as consisting of two passes. But we can inline calls to \texttt{makeT} into \texttt{merge} thus obtaining a single pass algorithm. This new algorithm will have two bases cases (as previously) and four inductive cases:

\begin{enumerate}
 \item (base case) \texttt{h1} is empty: return \texttt{h2}.
 \item (base case) \texttt{h2} is empty: return \texttt{h1}.
 \item (1st inductive case) priority \texttt{p1} is higher than \texttt{p2} and \textit{l1} is not smaller than  \textit{r1}$\oplus$\textit{h2}: \texttt{p1} becomes new root, \texttt{l1} becomes the left child and \texttt{r1}$\oplus$\texttt{h2} becomes the right child.
 \item (2nd inductive case) priority \texttt{p1} is higher than \texttt{p2} and \textit{r1}$\oplus$\textit{h2} is not smaller than \textit{l1}: \texttt{p1} becomes new root, \texttt{r1}$\oplus$\texttt{h2} becomes the left child and \texttt{l1} becomes the right child.
 \item (3rd inductive case) priority \texttt{p2} is higher or equal to \texttt{p1} and \textit{l2} is not smaller than  \textit{r2}$\oplus$\textit{h1}: \texttt{p2} becomes new root, \texttt{l2} becomes the left child and \texttt{r2}$\oplus$\texttt{h1} becomes the right child.
 \item (4th inductive case) priority \texttt{p2} is higher or equal to \texttt{p1} and \textit{r2}$\oplus$\textit{h1} is not smaller than  \textit{l2}: \texttt{p2} becomes new root, \texttt{r2}$\oplus$\texttt{h1} becomes the left child and \texttt{l2} becomes the right child.
\end{enumerate}
%Careful reader will notice that pairs of inductive cases overlap when both new children have the same rank (ie. $l1 ≡ r1 \oplus h2$ in cases 3 and 4; $l2 ≡ r2 \oplus h1$ in cases 5 and 6). % that is not really relevant
\noindent
Now that we have inlined \texttt{makeT} we must construct new proofs of \texttt{merge}. Note that previously we only made calls to \texttt{makeT} only in inductive cases. This means that implementation of base cases remains unchanged and so do the proofs. Let us now take a closer look at proofs we need to supply for inductive cases:

\begin{itemize}
 \item (1st inductive case): call to \texttt{makeT} would not swap left and right when creating a node from parameters passed to it. We must prove:

\begin{equation*}
\suc (l1 + (r1 + \suc (l2 + r2))) ≡ \suc ((l1 + r1) + \suc (l2 + r2))
\end{equation*}

\item (2nd inductive case): call to \texttt{makeT} would swap left and right when creating a node from parameters passed to it. We must prove:

\begin{equation*}
\suc ((r1 + \suc (l2 + r2)) + l1) ≡ \suc ((l1 + r1) + \suc (l2 + r2))
\end{equation*}

\item (3rd inductive case): call to \texttt{makeT} would not swap left and right when creating a node from parameters passed to it. We must prove:

\begin{equation*}
\suc (l2 + (r2  + \suc (l1 + r1))) ≡ \suc ((l1 + r1) + \suc (l2 + r2))
\end{equation*}

\item (4th inductive case): call to \texttt{makeT} would swap left and right when creating a node from parameters passed to it. We must prove:

\begin{equation*}
\suc ((r2 + \suc (l1 + r1)) + l2) ≡ \suc ((l1 + r1) + \suc (l2 + r2))
\end{equation*}
\end{itemize}

First thing to note is that inductive cases 1 and 3 require us to supply the same proofs as the ones we gave for inductive cases in two-pass merge. This means we can reuse old proofs. What about cases 2 and 4? One thing we could do is construct proofs of these properties using technique described in Section~\ref{sec:eq-proofs-using-trans}. This is left as an exercise to the reader. Here we will proceed in a different way.

Notice that properties we have to prove in cases 2 and 4 are very similar to properties 1 and 3. The only difference between 1 and 2 and between 3 and 4 is the order of parameters inside outer $\suc$ on the left hand side of equality. This is expected: in cases 2 and 4 we swap left and right subtree passed to node and this is directly reflected in the types.  Now, if we could prove that:

\begin{equation*}
\suc ((r1 + \suc (l2 + r2)) + l1) ≡ \suc (l1 + (r1 + \suc (l2 + r2)))
\end{equation*}
\noindent
and
\begin{equation*}
\suc ((r2 + \suc (l1 + r1)) + l2) ≡ \suc (l2 + (r2  + \suc (l1 + r1)))
\end{equation*}
\noindent
then we could use transitivity to combine these new proofs with proofs of inductive cases 1 and 3 (which are reused old proofs of two-passs merge). If we abstract the parameters in the above equalities we see that the property we need to prove in both cases is:

\begin{equation*}
\suc (a + b) ≡ \suc (b + a)
\end{equation*}
\noindent
And that happens to be \texttt{makeT-lemma} from Section~\ref{sec:twopass-makeT-proof}! New version of \texttt{merge} was created by inlining calls to \texttt{makeT} and now it turns out we can construct proofs of that implementation by composing proofs of \texttt{makeT} and \texttt{merge}. This is exactly the same technique that was developed in Section~\ref{sec:eq-proofs-using-trans} only this time it is used on a slightly larger scale. It leads to a very elegant solutions presented in module \texttt{SinglePassMerge.}\texttt{RankProof} of the companion code.

\section[Proving priority property]{Proving priority property\implementation{TwoPassMerge.PriorityProof}} \label{sec:priority-invariant}

To prove priority property we will apply technique demonstrated by Altenkirch, McBride and McKinna in Section 5.2 of ``Why Dependent Types Matter'' \cite{AltMcBMcK05}. We will index \texttt{Heap} with \texttt{Priority}\footnote{To keep things simple we forget about rank proof we conducted earlier -- in this section we once again store Rank explicitly in the \texttt{node} constructor.}. Index of value n says that ``this heap can store elements with priorities n or lower''\footnote{Remember that lower priority means larger \texttt{Nat}. So if relation $p ≥ n$ holds for priorities $p$ and $n$ then $n$ is a priority higher than $p$.}. In other words \texttt{Heap} indexed with 0 can store any priority, while \texttt{Heap} indexed with 3 can store priorities 3, 4 and lower, but can't store 0, 1 and 2. The new definition of \texttt{Heap} looks like this\footnote{Actual implementation in the companion code is slightly different. It uses sized types \cite{Abe08} to guide the termination checker in the \texttt{merge} function. This issue is orthogonal to proofs conducted here, hence I avoid sized types in the paper for the sake of simplicity.}:

\begin{code}
data Heap : Priority → Set where
  empty : \{n : Priority\} → Heap n
  node  : \{n : Priority\} → (p : Priority) → Rank → p ≥ n →
          Heap p → Heap p → Heap n
\end{code}
\noindent
As always \texttt{Heap} has two constructors. The \texttt{empty} constructor returns \texttt{Heap n}, where index n is not constrained in any way. This means that empty heap can be given any restriction on priorities of stored elements. The \texttt{node} constructor also creates \texttt{Heap n}, but this time \texttt{n} is constrained. If we store priority \texttt{p} in a node then:

\begin{enumerate}
 \item the resulting heap can only be restricted to store priorities at least as high as \texttt{p}. For example, if we create a node that stores priority 3 we cannot restrict the resulting heap to store priorities 4 and lower, because the fact that we store 3 in that node violates the restriction. This restriction is expressed by the \texttt{p ≥ n} parameter: if we can construct a value of type \texttt{p ≥ n} then existance of such value becomes a proof that \texttt{p} is greater-equal to \texttt{n}.
 \item children of a node can only be restricted to store priorities that are not higher than \texttt{p}. Example: if we restrict a node to store priorities 4 and lower we cannot restrict its children to store priorities 3 or higher. This restriction is expressed by index \texttt{p} in the subheaps passed to node constructor.
\end{enumerate}

Altenkirch, McKinna and McBride \cite{AltMcBMcK05} used this technique to prove correctness of merge sort for lists. In a weight biased leftist heap every path from root to a leaf is a sorted list so extending their technique to our case is traightforward. I will elide discussion of \texttt{merge} as there is nothing new there compared to Altenkirch's paper. I will insted focus on issue of creating singleton heaps and inserting elements into a heap as these cases now become interesting.

When creating a singleton heap -- ie. a heap storing exactly one element -- we have to answer a question: "what priorities can we later store in a singleton heap that we just created?". "Any" seems to be a reasonable answer, which means the resulting heap will be indexed with 0, meaning: "Priorities lower or equal to 0 can be stored in this Heap" (remember that any priority is lower or equal to 0). With such a liberal definition of singleton heap it is easy to write definition of insert by requiring that both input and output heap can store any priorities:

\begin{code}
singleton : (p : Priority) → Heap zero
singleton p = node p (suc zero) ge0 empty empty

insert : Priority → Heap zero → Heap zero
insert p h = merge (singleton p) h
\end{code}

But what if we want to insert into a heap that is not indexed with 0? One solution is to be liberal and ``promote'' that heap so that after insertion it can store elements with any priorities. Remember that priority restriction can always be loosened but it cannot be tightened easily. However such a liberal approach might not always be satisfactory. We might actually want to keep priority bounds as tight as possible. Let us explore that possibility.

We may begin by rewriting the singleton function:

\begin{code}
singletonB' : \{b : Priority\} → (p : Priority) → p ≥ b → Heap b
singletonB' p p≥b = node p one p≥b empty empty
\end{code}
\noindent
Now \texttt{singletonB'} allows to construct a heap containing a single element with priority \texttt{p}, but the whole heap is bounded by some \texttt{b}. To construct such a heap we must supply a proof that \texttt{p} can actually be stored in \texttt{Heap b}. We can now implement new insertion function:

\begin{small}
\begin{code}
insertB' : \{b : Priority\} → (p : Priority) → p ≥ b → Heap p → Heap b
insertB' p p≥b h = merge (singletonB' p p≥b) (liftBound p≥b h)
\end{code}
\end{small}
\noindent
where \texttt{liftBound} is a function that loosens the priority bound of a heap, given evidence that it is possible to do so (ie. that the new bound is less restrictive than the old one). But if we try to construct a heap using \texttt{insertB'} we quickly discover that it is useless:

\begin{code}
example-heap : Heap zero
example-heap = (insertB' (suc zero) ge0
               (insertB' zero \hilight{\{ \}?} empty))
\end{code}
\noindent
In the second call to \texttt{insertB'} we are required to supply a proof that $0 \ge 1$, which of course is not possible. The problem is that using the new \texttt{insertB'} function we can only lower the bound on the heap and thus insert the elements into the heap in decreasing order:

\begin{code}
example-heap : Heap zero
example-heap = (insertB' zero ge0
               (insertB' (suc zero) ge0 empty))
\end{code}
\noindent
This is a direct consequence of our requirement that the heap we are inserting into is restricted exactly by the priority of element we are inserting. The bottom line is: indexing \texttt{Heap} with priority allows to prove that \texttt{merge} maintains the priority invariant but otherwise it is not very helpful.